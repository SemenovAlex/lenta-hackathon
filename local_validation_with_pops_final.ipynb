{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb850a2c",
   "metadata": {},
   "source": [
    "# Локальная валидация метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdb096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования подхода из бейзлайна для тестирования модели и \n",
    "# расчета метрики через деление hist_data на трейн и валидацию\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "val_shown = pd.read_csv('val_shown.csv')\n",
    "val_hidden = pd.read_csv('val_hidden.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03470e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = train.groupby('item_id')['pav_order_id'].count().reset_index().sort_values('pav_order_id', ascending=False).head(100)\n",
    "pops.columns = ['item_id', 'p']\n",
    "pops['p'] = pops['p'] / train.pav_order_id.nunique()\n",
    "pops = list(zip(pops.item_id, pops.p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc34d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# из списка кандидатов по совстречаемости удаляем повторяющиеся item_id, сохраняя порядок\n",
    "def get_unique_recs(recs: list, basket: list, pops: list, top_n: int) -> list:\n",
    "    rec_dict = {}\n",
    "    counter = 0\n",
    "\n",
    "    for k, v in recs:\n",
    "        if k not in rec_dict and k not in basket:\n",
    "            rec_dict[k] = v\n",
    "            counter += 1\n",
    "        if counter == top_n:\n",
    "            break\n",
    "\n",
    "    if counter == top_n:\n",
    "        return list(rec_dict.keys())\n",
    "    \n",
    "    for k, v in pops:\n",
    "        if k not in rec_dict and k not in basket:\n",
    "            rec_dict[k] = v\n",
    "            counter += 1\n",
    "        if counter == top_n:\n",
    "            break\n",
    "\n",
    "    return list(rec_dict.keys())        \n",
    "    \n",
    "\n",
    "def rec_by_item(item_id: int, most_freq_dict: dict) -> list:\n",
    "    return most_freq_dict.get(item_id, None)\n",
    "\n",
    "\n",
    "# для каждого item_id соберем top_n самых часто встречающихся item_id, отсортируем по частоте и выберем уникальные\n",
    "def rec_by_basket(basket: list, most_freq_dict: dict, pops: list, top_n: int = 20) -> list:\n",
    "    \n",
    "    res = []\n",
    "    for item in basket:\n",
    "        recs = rec_by_item(item, most_freq_dict)\n",
    "        if recs is not None:\n",
    "            res += recs\n",
    "    \n",
    "    res = sorted(res, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return get_unique_recs(res, basket, pops, top_n)\n",
    "\n",
    "# метрики оцениваются для вектора релевантности. пример:\n",
    "# реальные item_id, которые приобрел покупатель: [1 ,4, 5, 69]\n",
    "# рекомендованные алгоритмом item_id: [4, 6, 7, 8, 1, 2, 67, 90]\n",
    "# тогда вектор релеватности будет выглядеть следующим образом: [1, 0, 0, 0, 1, 0, 0, 0]\n",
    "# и уже по не му будет расчитываться ndcg\n",
    "def dcg(\n",
    "    y_relevance: np.ndarray\n",
    ") -> float:\n",
    "    return np.sum([(2**i - 1) / np.log2(k + 1) for (k, i) in enumerate(y_relevance, start=1)])\n",
    "\n",
    "def ndcg(\n",
    "    y_relevance: np.ndarray,\n",
    "    k: int\n",
    ") -> float:\n",
    "    if y_relevance.sum() == 0:\n",
    "        return 0.0\n",
    "    DCG = dcg(y_relevance[:k])\n",
    "    IDCG = dcg(-np.sort(-y_relevance)[:k])\n",
    "    return DCG / IDCG\n",
    "\n",
    "def apply_relevance(x):\n",
    "    return [int(item in x['hidden_basket']) for item in x['preds']]\n",
    "\n",
    "def create_relevance(pred):\n",
    "    d = pred.copy()\n",
    "    d['hidden_basket'] = d['hidden_basket'].apply(set)\n",
    "    d = d.apply(apply_relevance, axis=1)\n",
    "    return d\n",
    "\n",
    "def ndcg_full_dataset(d):\n",
    "    dd = pd.DataFrame(d.to_list()).fillna(0).to_numpy()\n",
    "    k = dd.shape[1]\n",
    "    scores = [ndcg(dd[i], k) for i in range(len(dd))]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def compute_ndcg_score(pred):\n",
    "    relevance = create_relevance(pred)\n",
    "    return ndcg_full_dataset(relevance)\n",
    "\n",
    "def make_coocurs_dict(train_data):\n",
    "    tmp = (\n",
    "        train_data[['item_id', 'pav_order_id']]\n",
    "        .sort_values(['item_id', 'pav_order_id'])\n",
    "        .merge(train_data[['item_id', 'pav_order_id']], how='left', on=['pav_order_id'], suffixes=('', '_left'))\n",
    "    )\n",
    "    tmp = tmp[tmp['item_id'] != tmp['item_id_left']].copy()\n",
    "    tmp1 = tmp.groupby(['item_id'])['item_id_left'].agg(lambda x: Counter(x).most_common(10))\n",
    "    tmp2 = train_data.groupby(['item_id'])['pav_order_id'].count().reset_index()\n",
    "    base = dict(zip(tmp2.item_id, tmp2.pav_order_id))\n",
    "\n",
    "    most_freq_dict = {k: [(x[0], (x[1]+0.01)/(10+base[k])) for x in v] for (k, v) in tmp1.iteritems()}\n",
    "\n",
    "    del tmp1, tmp\n",
    "    gc.collect()\n",
    "    return most_freq_dict\n",
    "\n",
    "def create_basket(test_data):\n",
    "    basket = test_data.groupby(['pav_order_id'])['item_id'].agg([('basket', list)])\n",
    "    return basket\n",
    "\n",
    "def create_basket_with_hidden(test_data_shown, test_data_hidden):\n",
    "    basket = test_data_shown.groupby(['pav_order_id'])['item_id'].agg([('basket', list)])\n",
    "    hidden = test_data_hidden.groupby(['pav_order_id'])['item_id'].agg([('hidden_basket', list)])\n",
    "    basket['hidden_basket'] = hidden['hidden_basket']\n",
    "    return basket\n",
    "\n",
    "def make_predictions(test_data_shown, test_data_hidden, most_freq_dict, pops):\n",
    "    pred = create_basket_with_hidden(test_data_shown, test_data_hidden)\n",
    "    pred['preds'] = pred['basket'].map(lambda x: rec_by_basket(x, most_freq_dict=most_freq_dict, pops=pops))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123bb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# соберем словарь встречаемостей - какие item_id покупались чаще с каждым item_id \n",
    "most_freq_dict = make_coocurs_dict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50768ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказываем\n",
    "pred = make_predictions(val_shown, val_hidden, most_freq_dict, pops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3643144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30814848873154566\n"
     ]
    }
   ],
   "source": [
    "# посчитаем скор для всего набора предсказаний # 10 в нормировке, без товаров корзины, вероятности\n",
    "d_score = compute_ndcg_score(pred)\n",
    "print(d_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac94f76",
   "metadata": {},
   "source": [
    "# Расчет для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab5b5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_data = pd.read_csv('hist_data.csv')\n",
    "\n",
    "# соберем словарь встречаемостей - какие item_id покупались чаще с \n",
    "# каждым item_id \n",
    "tmp = (\n",
    "    hist_data[['item_id', 'pav_order_id']]\n",
    "    .sort_values(['item_id', 'pav_order_id'])\n",
    "    .merge(hist_data[['item_id', 'pav_order_id']], how='left', on=['pav_order_id'], suffixes=('', '_left'))\n",
    ")\n",
    "tmp = tmp[tmp['item_id'] != tmp['item_id_left']].copy()\n",
    "tmp1 = tmp.groupby(['item_id'])['item_id_left'].agg(lambda x: Counter(x).most_common(10))\n",
    "tmp2 = hist_data.groupby(['item_id'])['pav_order_id'].count().reset_index()\n",
    "base = dict(zip(tmp2.item_id, tmp2.pav_order_id))\n",
    "\n",
    "most_freq_dict_test = {k: [(x[0], (x[1]+0.1)/(10+base[k])) for x in v] for (k, v) in tmp1.iteritems()}\n",
    "\n",
    "del tmp1, tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fef099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pops_test = hist_data.groupby('item_id')['pav_order_id'].count().reset_index().sort_values('pav_order_id', ascending=False).head(100)\n",
    "pops_test.columns = ['item_id', 'p']\n",
    "pops_test['p'] = pops_test['p'] / hist_data.pav_order_id.nunique()\n",
    "pops_test = list(zip(pops_test.item_id, pops_test.p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea8eed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "pred = test.groupby(['pav_order_id'])['item_id'].agg([('basket', list)])\n",
    "pred['preds'] = pred['basket'].map(lambda x: rec_by_basket(x, most_freq_dict=most_freq_dict_test, pops=pops_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d597006",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    if len(pred.iloc[i]['preds'])!= 20:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94191c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['preds'].to_csv('pred.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
